# -*- coding: utf-8 -*-
"""Medicare Fraud Detection (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cDdNczD7h7CvqCKfEpb_rsPnuiUIZ0id
"""

import pandas as pd
import numpy as np
import scipy
import os 

import matplotlib.pyplot as plt


from sklearn.naive_bayes import GaussianNB 
from sklearn.linear_model import LogisticRegression 
from sklearn import ensemble 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import brier_score_loss, precision_score, recall_score,f1_score, roc_auc_score, accuracy_score 
from sklearn.metrics import confusion_matrix, roc_curve

from sklearn.preprocessing import StandardScaler 
from sklearn.feature_extraction import DictVectorizer
from sklearn.cluster import KMeans

import random

from scipy.stats import ttest_ind

PartDRawData = "c:\\FIP\\PartD_Prescriber_PUF_NPI_Drug_17.txt"

partD_pd = pd.read_csv(PartDRawData,sep="\t")

partD_pd.shape

partD_Drug_pd1= partD_pd.loc[:,['npi','nppes_provider_city','nppes_provider_state', \
                                               'nppes_provider_last_org_name', \
                                               'nppes_provider_first_name', \
                                               'specialty_description',\
                                               'drug_name', \
                                               'generic_name',\
                                               'total_drug_cost',\
                                               'total_claim_count',\
                                               'total_day_supply']]

partD_pd1 = partD_Drug_pd1

partD_Drug_pd = partD_pd.loc[:,['npi','drug_name','total_drug_cost','total_claim_count','total_day_supply','specialty_description']]
partD_Drug_pd['npi'] = partD_Drug_pd.npi.astype(object)

partD_Spec_pd1= partD_pd.loc[:,['npi','specialty_description']]

partD_Spec_pd1.head(0)

partD_Drug_pd.head()

partD_pd0= partD_pd.loc[:,['npi','nppes_provider_city','nppes_provider_state', \
                                               'nppes_provider_last_org_name', \
                                               'nppes_provider_first_name','specialty_description']]

partD_pd0.head()

partD_catfpd = partD_pd0.drop_duplicates()

partD_catfpd.head()

rename_dict = {'nppes_provider_first_name':'first_name', 'nppes_provider_last_org_name':'last_name','nppes_provider_city':'city','nppes_provider_state':'state','specialty_description':'Speciality'}
partD_catfpd = partD_catfpd.rename(columns=rename_dict)

partD_catfpd.head()

group_cols = ['npi']

agg_dict = {'total_drug_cost':['sum','mean','max'], \
           'total_claim_count':['sum','mean','max'],\
           'total_day_supply':['sum','mean','max']}



partD_pd2 = partD_pd1.groupby(group_cols).agg(agg_dict).astype(float)

partD_pd2.head(-10)

level0 = partD_pd2.columns.get_level_values(0)
level1 = partD_pd2.columns.get_level_values(1)
partD_pd2.columns = level0 + '_' + level1
partD_fpd = partD_pd2.reset_index()

partD_fpd.head()

partD_fpd.count()

partD_allpd = pd.merge(partD_fpd,partD_catfpd, how ='left',on = 'npi')

partD_allpd.head()

partD_allpd.count()

PaymentRawData = "c:\\FIP\\OP_DTL_GNRL_PGYR2017_P01172020.csv"

payment_pd = pd.read_csv(PaymentRawData)

payment_fpd = payment_pd.loc[:,['Physician_First_Name',\
                                             'Physician_Last_Name', \
                                             'Recipient_City', \
                                             'Recipient_State', \
                                             'Total_Amount_of_Payment_USDollars']]

payment_fpd.head()

payment_fpd.count()

payment_fpd1 = payment_fpd.groupby(['Physician_First_Name','Physician_Last_Name','Recipient_City','Recipient_State'])\
                                   .agg({'Total_Amount_of_Payment_USDollars':['sum']}).astype(float)

level0 = payment_fpd1.columns.get_level_values(0)

level1 = payment_fpd1.columns.get_level_values(1)

payment_fpd1.columns = level0 + '_' + level1

payment_fpd1.reset_index()

payment_fpd1.head()

rename_dict = {'Physician_First_Name':'first_name', 'Physician_Last_Name':'last_name','Recipient_City':'city','Recipient_State':'state','Total_Amount_of_Payment_USDollars_sum':'Total_Payment_Sum'}
payment_fpd1 = payment_fpd1.rename(columns=rename_dict)

payment_fpd1.head()

payment_fpd2= payment_fpd1.reset_index()

rename_dict = {'Physician_First_Name':'first_name', 'Physician_Last_Name':'last_name','Recipient_City':'city','Recipient_State':'state','Total_Amount_of_Payment_USDollars_sum':'Total_Payment_Sum'}
payment_fpd2 = payment_fpd2.rename(columns=rename_dict)

payment_fpd2 = payment_fpd2.sort_values('Total_Payment_Sum',ascending=False)

payment_fpd2.head()

print(payment_fpd2.dtypes)

payment_fpd2.apply(lambda x: x.astype(str).str.upper())

payment_fpd2.head()

pay_partD_fpd = pd.merge (partD_allpd,payment_fpd2, how ='left', on = ['last_name','first_name','city','state'])

pay_partD_fpd.head()

pay_partD_fpd.count()

IELErawdata = "c:\\FIP\\LEIE.csv"
IELE_pd = pd.read_csv(IELErawdata)

IELE_pd.head()

npifraud_pd0 = IELE_pd.loc[:,['NPI','EXCLTYPE']]

npifraud_pd0.head()

npifraud_pd1 = npifraud_pd0.query('NPI !=0')

npifraud_pd1.count()

rename_dict = {'NPI':'npi', 'EXCLTYPE':'is_fraud'}
npi_fraud_pd = npifraud_pd1.rename(columns=rename_dict)

npi_fraud_pd.head()

npi_fraud_pd['is_fraud'] = 1

npi_fraud_pd.head()

print(npi_fraud_pd.dtypes)

# Features Engineering 
Features_pd1 = pd.merge(pay_partD_fpd,npi_fraud_pd, how ='left',on = 'npi')

Features_pd1.count()

Features_pd1.describe()

Features_pd1.fillna(0, inplace=True)

Features_pd1

Features_pd1[Features_pd1['is_fraud']==1].count()

FeaturesAll_pd=Features_pd1

# Scaling the features
FeaturesAll_pd['total_drug_cost_sum'] = FeaturesAll_pd['total_drug_cost_sum'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['total_claim_count_sum'] = FeaturesAll_pd['total_claim_count_sum'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['total_day_supply_sum'] = FeaturesAll_pd['total_day_supply_sum'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['Total_Payment_Sum'] = FeaturesAll_pd['Total_Payment_Sum'].map(lambda x: np.log10(x + 1.0))

FeaturesAll_pd['total_drug_cost_mean'] = FeaturesAll_pd['total_drug_cost_mean'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['total_claim_count_mean'] = FeaturesAll_pd['total_claim_count_mean'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['total_day_supply_mean'] = FeaturesAll_pd['total_day_supply_mean'].map(lambda x: np.log10(x + 1.0))

FeaturesAll_pd['total_drug_cost_max'] = FeaturesAll_pd['total_drug_cost_max'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['total_claim_count_max'] = FeaturesAll_pd['total_claim_count_max'].map(lambda x: np.log10(x + 1.0))
FeaturesAll_pd['total_day_supply_max'] = FeaturesAll_pd['total_day_supply_max'].map(lambda x: np.log10(x + 1.0))


FeaturesAll_pd['claim_max-mean'] = FeaturesAll_pd['total_claim_count_max'] - FeaturesAll_pd['total_claim_count_mean']

FeaturesAll_pd['supply_max-mean'] = FeaturesAll_pd['total_day_supply_max'] - FeaturesAll_pd['total_day_supply_mean']

FeaturesAll_pd['drug_max-mean'] = FeaturesAll_pd['total_drug_cost_max'] - FeaturesAll_pd['total_drug_cost_mean']

FeaturesAll_pd

FeaturesAll_pd['npi'] = FeaturesAll_pd.npi.astype(object)

categorical_features = ['npi','last_name', 'Speciality','first_name','city', 'state']

numerical_features = ['total_drug_cost_sum', 'total_drug_cost_mean','Total_Payment_Sum',
       'total_drug_cost_max', 'total_claim_count_sum',
       'total_claim_count_mean', 'total_claim_count_max',
       'total_day_supply_sum', 'total_day_supply_mean', 'total_day_supply_max',
    'claim_max-mean','supply_max-mean', 'drug_max-mean']

target = ['is_fraud']

allvars = categorical_features + numerical_features + target

y = FeaturesAll_pd["is_fraud"].values
X = FeaturesAll_pd[allvars].drop('is_fraud',axis=1)

# scikit learn 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
#from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder 
from sklearn.feature_extraction import DictVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_curve, auc

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)
print(X_train.shape)
print(X_valid.shape)

X_train[numerical_features] = X_train.loc[:,numerical_features].fillna(0) 
X_valid[numerical_features] = X_valid.loc[:,numerical_features].fillna(0) 
X_train[categorical_features] = X_train.loc[:,categorical_features].fillna('NA') 
X_valid[categorical_features] = X_valid.loc[:,categorical_features].fillna('NA')

scaler= StandardScaler() 
X_train[numerical_features] = scaler.fit_transform(X_train[numerical_features].values)
X_valid[numerical_features] = scaler.transform(X_valid[numerical_features].values)

print(X_train[numerical_features].dtypes)

ix_ran = FeaturesAll_pd.index.values
np.random.shuffle(ix_ran)

df_len = len(FeaturesAll_pd)
train_len = int(df_len * 0.8)  # 80% for training


ix_train = ix_ran[:train_len]
ix_valid = ix_ran[train_len:]

df_train = FeaturesAll_pd.ix[ix_train]
df_valid = FeaturesAll_pd.ix[ix_valid]

print(len(ix_train))
print(len(ix_valid))

print(df_train.dtypes)

# Drug Weighted_Scores

partD_drug_train = pd.merge(partD_Drug_pd,df_train[['npi','is_fraud']], how='inner', on=['npi'])
partD_drug_All = pd.merge(partD_Drug_pd,FeaturesAll_pd[['npi','is_fraud']], how='inner', on=['npi'])

print(len(partD_drug_train[partD_drug_train['is_fraud']==1]))

# get unique drug names
drugs = set([ drugx for drugx in partD_drug_train['drug_name'].values if isinstance(drugx, str)])
print(len(drugs))

print("Total records in train set : ")
print(len(partD_drug_train))
print("Total Fraud in train set : ")
print(len(partD_drug_train[partD_drug_train['is_fraud']==1]))
partD_drug_train.head()

cols = ['total_drug_cost','total_claim_count','total_day_supply']

partD_drug_train_Group = partD_drug_train.groupby(['drug_name', 'is_fraud'])
partD_drug_All_Group = partD_drug_All.groupby(['drug_name', 'is_fraud'])

drug_keys = partD_drug_train_Group.groups.keys()
print(len(drug_keys))

drug_keys

drug_with_isfraud = [drugx for drugx in drugs if ((drugx,0.0) in drug_keys ) & ( (drugx,1.0) in drug_keys)]

from scipy.stats import ttest_ind
re_drug_tt = dict()
for drugx in drug_with_isfraud:
    for colx in cols:
        fraud_0 = partD_drug_train_Group.get_group((drugx,0.0))[colx].values
        fraud_1 = partD_drug_train_Group.get_group((drugx,1.0))[colx].values
        # print len(fraud_0), len(fraud_1)
        if (len(fraud_0)>2) & (len(fraud_1)>2) :
            tt = ttest_ind(fraud_0, fraud_1)
            re_drug_tt[(drugx, colx)] = tt

#Setting Probilities
Prob_005 = [(key, p) for (key, (t, p)) in re_drug_tt.items() if p <=0.05]  
print(len(Prob_005))

inx=100
drug_name = Prob_005[inx][0][0]
print(drug_name)
df_bar = pd.concat([partD_drug_All_Group.get_group((Prob_005[inx][0][0],0.0)), partD_drug_All_Group.get_group((Prob_005[inx][0][0],1.0))])
df_bar.head()

Feture_DrugWeighted = []
new_col_all =[]
for i, p005x in enumerate(Prob_005):
    #if i>4:
    #   break
    drug_name = p005x[0][0]
    cat_name = p005x[0][1] 
    
    new_col = drug_name+'_'+cat_name
    new_col_all.append(new_col)

    drug_0 = partD_drug_All_Group.get_group((drug_name,0.0))[['npi', cat_name]]
    drug_1 = partD_drug_All_Group.get_group((drug_name,1.0))[['npi', cat_name]]

    drug_01 = pd.concat([drug_0, drug_1])
    drug_01.rename(columns={cat_name: new_col}, inplace=True)
    Feture_DrugWeighted.append(drug_01)

npi_col = FeaturesAll_pd[['npi']]

w_npi = []

for n, nx in enumerate(Feture_DrugWeighted):
      nggx = pd.merge(npi_col, nx.drop_duplicates(['npi']), on='npi', how='left')
      w_npi.append(nggx)

FeaturesAll_pd1 = FeaturesAll_pd

for wx in w_npi:
    col_n = wx.columns[1]
    FeaturesAll_pd1[col_n] = wx[col_n].values
    
wx = w_npi[0]
wx.columns[1]
col_n = wx.columns[1]

len(wx[col_n].values)
FeaturesAll_pd1.fillna(0)

new_col_all

FeaturesAll_pd1[new_col_all].describe()

FeaturesAll_pd1['drug_mean'] = FeaturesAll_pd1[new_col_all].mean(axis=1)

FeaturesAll_pd['drug_mean'] = FeaturesAll_pd['drug_mean'].map(lambda x: np.log10(x + 1.0))

FeaturesAll_pd1['drug_sum'] = FeaturesAll_pd1[new_col_all].sum(axis=1)
FeaturesAll_pd['drug_sum'] = FeaturesAll_pd['drug_sum'].map(lambda x: np.log10(x + 1.0))

FeaturesAll_pd1['drug_variance'] = FeaturesAll_pd1[new_col_all].var(axis=1)

FeaturesAll_pd1





df_train = FeaturesAll_pd1.ix[ix_train]
df_valid = FeaturesAll_pd1.ix[ix_valid]

df_train.fillna(0)
df_valid.fillna(0)

df_valid.columns

#Create the Specialty Weight
spec_dict =[]
spec_fraud_1 = df_train[df_train['is_fraud']==1]['Speciality']

from collections import Counter
counts = Counter(spec_fraud_1)
spec_dict =  dict(counts)

FeaturesAll_pd1['Spec_Weight'] = FeaturesAll_pd1['Speciality'].map(lambda x: spec_dict.get(x, 0))

df_train = FeaturesAll_pd1.ix[ix_train]
df_valid = FeaturesAll_pd1.ix[ix_valid]

len(df_train[df_train['is_fraud'] == 1])

print(df_train.dtypes)

df_train.fillna(0)

df_valid.fillna(0)

numerical_features1 = numerical_features + ['drug_sum','Spec_Weight']

numerical_features1



positives=len(df_train[df_train['is_fraud'] == 1])
positives

dataset_size=len(df_train)
dataset_size

per_ones=(float(positives)/float(dataset_size))*100
per_ones

negatives=float(dataset_size-positives)
t=negatives/positives
t

BalancingRatio= positives/dataset_size
BalancingRatio

BalancingRatio= positives/dataset_size
BalancingRatio



X= df_train[numerical_features1].values
Y = df_train['is_fraud'].values
clf =  LogisticRegression(C=1e5, class_weight={0:1, 1:4000}, n_jobs=3)
clf.fit(X,Y)
y_p=clf.predict_proba(X)

params_0 = {'n_estimators': 100, 'max_depth': 8, 'min_samples_split': 3, 'learning_rate': 0.01}
params_1 = {'n_estimators': 500, 'max_depth': 10, 'min_samples_split': 5, 'class_weight' : {0:1, 1:2514}, 'n_jobs':5}

scaler = StandardScaler()
    
clfs = [
    LogisticRegression(C=1e5,class_weight= {0:1, 1:2514}, n_jobs=5),
    
    GaussianNB(),

    ensemble.RandomForestClassifier(**params_1),

    ensemble.ExtraTreesClassifier(**params_1),
    
    ensemble.GradientBoostingClassifier(**params_0)
    
    ]

X_train = df_train[numerical_features1].values

y_train = df_train['is_fraud'].values
    
X_train = scaler.fit_transform(X_train)

X_valid = df_valid[numerical_features1].values
y_valid = df_valid['is_fraud'].values
X_valid_x= scaler.transform(X_valid)

prob_result = []
df_m = []
clfs_fited = []
for clf in clfs:
    print("%s:" %  clf.__class__.__name__)
    clf.fit(X_train,y_train)
    clfs_fited.append(clf)
    y_pred = clf.predict(X_valid_x)
    prob_pos  = clf.predict_proba(X_valid_x)[:, 1]
    prob_result.append(prob_pos)
    m = confusion_matrix(y_valid, y_pred)
    clf_score = brier_score_loss(y_valid, prob_pos, pos_label=y_valid.max())
    print("\tBrier: %1.5f" % (clf_score))
    print("\tPrecision: %1.5f" % precision_score(y_valid, y_pred))
    print("\tRecall: %1.5f" % recall_score(y_valid, y_pred))
    print("\tF1: %1.5f" % f1_score(y_valid, y_pred))
    print("\tauc: %1.5f" % roc_auc_score(y_valid, prob_pos))
    print("\tAccuracy: %1.5f\n" % accuracy_score(y_valid, y_pred))
    df_m.append(
        pd.DataFrame(m, index=['True Negative', 'True Positive'], columns=['Pred. Negative', 'Pred. Positive'])
        )

fpr, tpr, thresholds = roc_curve(y_valid, prob_result[2])

fpr, tpr, thresholds = roc_curve(y_valid, prob_result[2])
roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, lw=1, label='ROC (area = %0.2f)' % roc_auc)
plt.xlim([-0.05, 1.05])
plt.ylim([-0.05, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.show()

m

X_valid_x[0]

df_train

X_valid_x[1]

y_pred = clf.predict(X_valid_x)

y_pred

X_train[0]

feature_importance = clfs_fited[2].feature_importances_
# make importances relative to max importance
feature_importance = 100.0 * (feature_importance / feature_importance.max())
sorted_idx = np.argsort(feature_importance)

feature_importance[sorted_idx]

features = [numerical_features1[ix] for ix in sorted_idx]
bardata = {"name":features[::-1], "importance percent":feature_importance[sorted_idx][::-1]}

plt.figure()

# Create plot title
plt.title("Feature Importance")

# Add bars
plt.bar(range(X.shape[1]), feature_importance[sorted_idx])

# Add feature names as x-axis labels
plt.xticks(range(X.shape[1]), features, rotation=90)

# Show plot
plt.show()

